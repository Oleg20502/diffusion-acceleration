{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEIS Sampling Method Testing\n",
    "\n",
    "This notebook tests the **DEIS (Diffusion Exponential Integrator Sampler)** method using `DEISMultistepScheduler` from the diffusers library.\n",
    "\n",
    "DEIS is a fast high-order solver for diffusion ODEs that can generate high-quality samples with fewer function evaluations.\n",
    "\n",
    "## Hyperparameters tested:\n",
    "- **Number of inference steps** (5, 10, 20, 50, 100)\n",
    "- **Solver order** (1, 2, 3)\n",
    "- **Beta schedule** (linear, scaled_linear, squaredcos_cap_v2)\n",
    "- **Solver type** (logrho, midpoint, heun, bh1, bh2)\n",
    "- **Lower order final** (True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n",
      "    return await f(*args, **kwargs)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2271208/3909407996.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'xpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mema_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EMA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_grid, save_image\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEISMultistepScheduler\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unet, GaussianDiffusion\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Set device\u001b[39;00m\n",
      "File \u001b[0;32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/diffusers/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.36.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     DIFFUSERS_SLOW_IMPORT,\n\u001b[1;32m      7\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m      8\u001b[0m     _LazyModule,\n\u001b[1;32m      9\u001b[0m     is_accelerate_available,\n\u001b[1;32m     10\u001b[0m     is_bitsandbytes_available,\n\u001b[1;32m     11\u001b[0m     is_flax_available,\n\u001b[1;32m     12\u001b[0m     is_gguf_available,\n\u001b[1;32m     13\u001b[0m     is_k_diffusion_available,\n\u001b[1;32m     14\u001b[0m     is_librosa_available,\n\u001b[1;32m     15\u001b[0m     is_note_seq_available,\n\u001b[1;32m     16\u001b[0m     is_nvidia_modelopt_available,\n\u001b[1;32m     17\u001b[0m     is_onnx_available,\n\u001b[1;32m     18\u001b[0m     is_opencv_available,\n\u001b[1;32m     19\u001b[0m     is_optimum_quanto_available,\n\u001b[1;32m     20\u001b[0m     is_scipy_available,\n\u001b[1;32m     21\u001b[0m     is_sentencepiece_available,\n\u001b[1;32m     22\u001b[0m     is_torch_available,\n\u001b[1;32m     23\u001b[0m     is_torchao_available,\n\u001b[1;32m     24\u001b[0m     is_torchsde_available,\n\u001b[1;32m     25\u001b[0m     is_transformers_available,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n\u001b[1;32m     36\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguiders\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     ],\n\u001b[1;32m     65\u001b[0m }\n",
      "File \u001b[0;32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/diffusers/utils/__init__.py:131\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOutput\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     check_peft_version,\n\u001b[1;32m    133\u001b[0m     delete_adapter_layers,\n\u001b[1;32m    134\u001b[0m     get_adapter_name,\n\u001b[1;32m    135\u001b[0m     get_peft_kwargs,\n\u001b[1;32m    136\u001b[0m     recurse_remove_peft_layers,\n\u001b[1;32m    137\u001b[0m     scale_lora_layers,\n\u001b[1;32m    138\u001b[0m     set_adapter_layers,\n\u001b[1;32m    139\u001b[0m     set_weights_and_activate_adapters,\n\u001b[1;32m    140\u001b[0m     unscale_lora_layers,\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpil_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PIL_INTERPOLATION, make_image_grid, numpy_to_pil, pt_to_pil\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m remote_decode\n",
      "File \u001b[0;32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/diffusers/utils/peft_utils.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_peft_available, is_peft_version, is_torch_available\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m empty_device_cache\n\u001b[1;32m     29\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "File \u001b[0;32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/diffusers/utils/torch_utils.py:33\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fftn, fftshift, ifftn, ifftshift\n\u001b[1;32m     30\u001b[0m BACKEND_SUPPORTS_TRAINING \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m     31\u001b[0m BACKEND_EMPTY_CACHE \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache,\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpu\u001b[49m\u001b[38;5;241m.\u001b[39mempty_cache,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m }\n\u001b[1;32m     38\u001b[0m BACKEND_DEVICE_COUNT \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mdevice_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     45\u001b[0m BACKEND_MANUAL_SEED \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed,\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mmanual_seed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmanual_seed,\n\u001b[1;32m     51\u001b[0m }\n",
      "File \u001b[0;32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.10/site-packages/torch/__init__.py:1833\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'xpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "from ema_pytorch import EMA\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from diffusers import DEISMultistepScheduler\n",
    "\n",
    "from diffusion.ddpm import Unet, GaussianDiffusion\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (should match training config)\n",
    "IMAGE_SIZE = 28\n",
    "CHANNELS = 3\n",
    "TIMESTEPS = 1000\n",
    "\n",
    "# Initialize model architecture\n",
    "model = Unet(\n",
    "    dim=64,\n",
    "    dim_mults=(1, 2, 4),\n",
    "    flash_attn=False,\n",
    "    channels=CHANNELS\n",
    ")\n",
    "\n",
    "# Create diffusion wrapper (needed for loading checkpoint)\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    timesteps=TIMESTEPS,\n",
    "    sampling_timesteps=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Evaluation Setup\n",
    "\n",
    "We'll use FID (FrÃ©chet Inception Distance) to quantitatively compare sample quality across different DEIS configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from a folder for FID computation.\"\"\"\n",
    "    def __init__(self, root, image_size=299):\n",
    "        self.root = Path(root)\n",
    "        exts = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"]\n",
    "        self.paths = sorted(\n",
    "            p for ext in exts for p in self.root.rglob(f\"*{ext}\")\n",
    "        )\n",
    "\n",
    "        if len(self.paths) == 0:\n",
    "            raise RuntimeError(f\"No images found in {self.root}\")\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "        self.resize = T.Resize((image_size, image_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")  \n",
    "        img = self.resize(img)\n",
    "        img = T.functional.to_tensor(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_fid(\n",
    "    real_dir: str,\n",
    "    fake_dir: str,\n",
    "    batch_size: int = 64,\n",
    "    device: str = None,\n",
    "    num_workers: int = 4,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute FID score between real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        real_dir: Path to directory containing real images\n",
    "        fake_dir: Path to directory containing generated images\n",
    "        batch_size: Batch size for processing\n",
    "        device: Device to use for computation\n",
    "        num_workers: Number of workers for data loading\n",
    "    \n",
    "    Returns:\n",
    "        FID score (lower is better)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    real_ds = ImageFolderDataset(real_dir)\n",
    "    fake_ds = ImageFolderDataset(fake_dir)\n",
    "\n",
    "    real_loader = DataLoader(\n",
    "        real_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    fake_loader = DataLoader(\n",
    "        fake_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "    fid.eval()\n",
    "\n",
    "    for imgs in real_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        fid.update(imgs, real=True)\n",
    "\n",
    "    for imgs in fake_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        fid.update(imgs, real=False)\n",
    "\n",
    "    value = fid.compute().item()\n",
    "    return value\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_fid_from_tensors(\n",
    "    real_images: torch.Tensor,\n",
    "    fake_images: torch.Tensor,\n",
    "    device: str = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute FID score directly from image tensors.\n",
    "    \n",
    "    Args:\n",
    "        real_images: Tensor of real images (N, C, H, W) in [0, 1]\n",
    "        fake_images: Tensor of generated images (N, C, H, W) in [0, 1]\n",
    "        device: Device to use for computation\n",
    "    \n",
    "    Returns:\n",
    "        FID score (lower is better)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
    "    fid.eval()\n",
    "    \n",
    "    # Resize to 299x299 for Inception\n",
    "    resize = T.Resize((299, 299), antialias=True)\n",
    "    \n",
    "    # Process real images in batches\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(real_images), batch_size):\n",
    "        batch = real_images[i:i+batch_size].to(device)\n",
    "        batch = resize(batch)\n",
    "        # Convert to uint8 [0, 255] as expected by FID\n",
    "        batch = (batch * 255).to(torch.uint8)\n",
    "        fid.update(batch, real=True)\n",
    "    \n",
    "    # Process fake images in batches\n",
    "    for i in range(0, len(fake_images), batch_size):\n",
    "        batch = fake_images[i:i+batch_size].to(device)\n",
    "        batch = resize(batch)\n",
    "        batch = (batch * 255).to(torch.uint8)\n",
    "        fid.update(batch, real=False)\n",
    "    \n",
    "    value = fid.compute().item()\n",
    "    return value\n",
    "\n",
    "\n",
    "def save_samples_to_dir(samples: torch.Tensor, output_dir: Path, prefix: str = \"sample\"):\n",
    "    \"\"\"Save tensor samples as individual images to a directory.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, img in enumerate(samples):\n",
    "        img_path = output_dir / f\"{prefix}_{i:04d}.png\"\n",
    "        save_image(img, img_path)\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1080 Ti which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU1 NVIDIA GeForce GTX 1080 Ti which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU2 NVIDIA GeForce GTX 1080 Ti which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1080 Ti with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[32m      2\u001b[39m CKPT_PATH = \u001b[33m\"\u001b[39m\u001b[33m./ckpts/model-5.pt\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Update this path to your checkpoint\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m diffusion.load_state_dict(ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Setup EMA model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:1530\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1538\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:2122\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2120\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2121\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2122\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2123\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2125\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:2086\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2084\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2085\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2086\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:2052\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2048\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   2049\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._guards.detect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2052\u001b[39m     wrap_storage = \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2054\u001b[39m     storage._fake_device = location\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:1859\u001b[39m, in \u001b[36m_get_restore_location.<locals>.restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrestore_location\u001b[39m(storage, location):\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:698\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    680\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    696\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    700\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/serialization.py:637\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m    636\u001b[39m     device = _validate_device(location, backend_name)\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/storage.py:291\u001b[39m, in \u001b[36m_StorageBase.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch.device):\n\u001b[32m    290\u001b[39m     device = torch.device(device)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/home/admin/miniforge3/envs/dif-accel/lib/python3.11/site-packages/torch/_utils.py:101\u001b[39m, in \u001b[36m_to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_sparse, (\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     untyped_storage = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     untyped_storage.copy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "CKPT_PATH = \"./ckpts/model-5.pt\"  # Update this path to your checkpoint\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device, weights_only=False)\n",
    "diffusion.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "# Setup EMA model\n",
    "ema = EMA(diffusion, beta=0.995, update_every=10).to(device)\n",
    "ema.load_state_dict(ckpt[\"ema\"])\n",
    "\n",
    "# Get the EMA model for inference\n",
    "ema_model = ema.ema_model\n",
    "ema_model.eval()\n",
    "\n",
    "# Extract the UNet from the diffusion model\n",
    "unet = ema_model.model\n",
    "unet.eval()\n",
    "\n",
    "print(f\"Model loaded from: {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEIS Sampling Implementation\n",
    "\n",
    "We create a sampling function that uses the `DEISMultistepScheduler` with our trained UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deis_scheduler(\n",
    "    num_train_timesteps: int = 1000,\n",
    "    beta_schedule: str = \"scaled_linear\",\n",
    "    beta_start: float = 0.0001,\n",
    "    beta_end: float = 0.02,\n",
    "    solver_order: int = 2,\n",
    "    algorithm_type: str = \"deis\",\n",
    "    solver_type: str = \"logrho\",\n",
    "    lower_order_final: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DEIS scheduler with specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        num_train_timesteps: Number of training timesteps\n",
    "        beta_schedule: Type of beta schedule ('linear', 'scaled_linear', 'squaredcos_cap_v2')\n",
    "        beta_start: Starting beta value\n",
    "        beta_end: Ending beta value\n",
    "        solver_order: Order of the DEIS solver (1, 2, or 3)\n",
    "        algorithm_type: Algorithm type ('deis' or 'dpmsolver++')\n",
    "        solver_type: Solver type ('logrho', 'midpoint', 'heun', 'bh1', 'bh2')\n",
    "        lower_order_final: Use lower order solver at final steps\n",
    "    \"\"\"\n",
    "    scheduler = DEISMultistepScheduler(\n",
    "        num_train_timesteps=num_train_timesteps,\n",
    "        beta_schedule=beta_schedule,\n",
    "        beta_start=beta_start,\n",
    "        beta_end=beta_end,\n",
    "        solver_order=solver_order,\n",
    "        algorithm_type=algorithm_type,\n",
    "        solver_type=solver_type,\n",
    "        lower_order_final=lower_order_final,\n",
    "        prediction_type=\"epsilon\",  # Our model predicts noise\n",
    "    )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample_with_deis(\n",
    "    unet,\n",
    "    scheduler,\n",
    "    num_inference_steps: int = 20,\n",
    "    batch_size: int = 16,\n",
    "    image_size: int = 28,\n",
    "    channels: int = 3,\n",
    "    device: str = 'cuda',\n",
    "    return_intermediates: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample images using DEIS scheduler.\n",
    "    \n",
    "    Args:\n",
    "        unet: The denoising UNet model\n",
    "        scheduler: DEIS scheduler instance\n",
    "        num_inference_steps: Number of denoising steps\n",
    "        batch_size: Number of images to generate\n",
    "        image_size: Size of generated images\n",
    "        channels: Number of image channels\n",
    "        device: Device to use\n",
    "        return_intermediates: Whether to return intermediate samples\n",
    "    \n",
    "    Returns:\n",
    "        samples: Generated images (batch_size, channels, image_size, image_size)\n",
    "        intermediates: List of intermediate samples (if return_intermediates=True)\n",
    "        elapsed_time: Time taken for sampling\n",
    "    \"\"\"\n",
    "    # Set number of inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = scheduler.timesteps\n",
    "    \n",
    "    # Initialize with random noise\n",
    "    latents = torch.randn(\n",
    "        (batch_size, channels, image_size, image_size),\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Scale initial noise by the scheduler's init_noise_sigma\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    \n",
    "    intermediates = [latents.clone()] if return_intermediates else None\n",
    "    \n",
    "    # Denoising loop\n",
    "    for t in tqdm(timesteps, desc=\"DEIS Sampling\", leave=False):\n",
    "        # Create batched timesteps\n",
    "        t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = unet(latents, t_batch)\n",
    "        \n",
    "        # DEIS step\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        if return_intermediates:\n",
    "            intermediates.append(latents.clone())\n",
    "    \n",
    "    # Unnormalize from [-1, 1] to [0, 1]\n",
    "    samples = (latents + 1) * 0.5\n",
    "    samples = samples.clamp(0, 1)\n",
    "    \n",
    "    if return_intermediates:\n",
    "        intermediates = [(x + 1) * 0.5 for x in intermediates]\n",
    "        return samples, intermediates\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(samples, title=\"Generated Samples\", nrow=4, figsize=(10, 10)):\n",
    "    \"\"\"Display a grid of generated samples.\"\"\"\n",
    "    grid = make_grid(samples, nrow=nrow, padding=2, normalize=False)\n",
    "    grid = grid.cpu().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_denoising_process(intermediates, num_steps_to_show=8, nrow=8, figsize=(16, 4)):\n",
    "    \"\"\"Visualize the denoising process.\"\"\"\n",
    "    n_intermediates = len(intermediates)\n",
    "    indices = np.linspace(0, n_intermediates - 1, num_steps_to_show, dtype=int)\n",
    "    \n",
    "    selected = [intermediates[i][0:1] for i in indices]  # Take first sample\n",
    "    selected = torch.cat(selected, dim=0)\n",
    "    \n",
    "    grid = make_grid(selected, nrow=nrow, padding=2, normalize=False)\n",
    "    grid = grid.cpu().permute(1, 2, 0).numpy().clip(0, 1)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid)\n",
    "    plt.title(\"Denoising Process (Noise -> Image)\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_results(results_dict, figsize=(16, 12)):\n",
    "    \"\"\"Compare samples from different configurations.\"\"\"\n",
    "    n_configs = len(results_dict)\n",
    "    fig, axes = plt.subplots(n_configs, 1, figsize=figsize)\n",
    "    \n",
    "    if n_configs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (name, data) in zip(axes, results_dict.items()):\n",
    "        samples = data['samples'][:8]  # Show first 8 samples\n",
    "        grid = make_grid(samples, nrow=8, padding=2, normalize=False)\n",
    "        grid = grid.cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        ax.imshow(grid)\n",
    "        ax.set_title(f\"{name}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Varying Number of Inference Steps\n",
    "\n",
    "Test how sample quality changes with different numbers of denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of inference steps\n",
    "step_counts = [5, 10, 20, 50, 100]\n",
    "\n",
    "results_steps = {}\n",
    "\n",
    "for num_steps in step_counts:\n",
    "    print(f\"\\nSampling with {num_steps} steps...\")\n",
    "    \n",
    "    scheduler = create_deis_scheduler(\n",
    "        num_train_timesteps=TIMESTEPS,\n",
    "        solver_order=2,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "    )\n",
    "    \n",
    "    samples = sample_with_deis(\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "        num_inference_steps=num_steps,\n",
    "        batch_size=16,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        channels=CHANNELS,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    results_steps[f\"{num_steps} steps\"] = {\n",
    "        'samples': samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results(results_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Varying Solver Order\n",
    "\n",
    "DEIS supports solver orders 1, 2, and 3. Higher orders can be more accurate but may be less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different solver orders\n",
    "solver_orders = [1, 2, 3]\n",
    "NUM_STEPS = 20\n",
    "\n",
    "results_orders = {}\n",
    "\n",
    "for order in solver_orders:\n",
    "    print(f\"\\nSampling with solver order {order}...\")\n",
    "    \n",
    "    scheduler = create_deis_scheduler(\n",
    "        num_train_timesteps=TIMESTEPS,\n",
    "        solver_order=order,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "    )\n",
    "    \n",
    "    samples, elapsed = sample_with_deis(\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "        num_inference_steps=NUM_STEPS,\n",
    "        batch_size=16,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        channels=CHANNELS,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    results_orders[f\"Order {order}\"] = {\n",
    "        'samples': samples,\n",
    "        'time': elapsed\n",
    "    }\n",
    "    print(f\"  Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "compare_results(results_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Varying Beta Schedule\n",
    "\n",
    "Test different noise schedules: linear, scaled_linear, and squaredcos_cap_v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different beta schedules\n",
    "beta_schedules = [\"linear\", \"scaled_linear\", \"squaredcos_cap_v2\"]\n",
    "NUM_STEPS = 20\n",
    "\n",
    "results_beta = {}\n",
    "\n",
    "for beta_sched in beta_schedules:\n",
    "    print(f\"\\nSampling with beta_schedule='{beta_sched}'...\")\n",
    "    \n",
    "    scheduler = create_deis_scheduler(\n",
    "        num_train_timesteps=TIMESTEPS,\n",
    "        solver_order=2,\n",
    "        beta_schedule=beta_sched,\n",
    "    )\n",
    "    \n",
    "    samples, elapsed = sample_with_deis(\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "        num_inference_steps=NUM_STEPS,\n",
    "        batch_size=16,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        channels=CHANNELS,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    results_beta[beta_sched] = {\n",
    "        'samples': samples,\n",
    "        'time': elapsed\n",
    "    }\n",
    "    print(f\"  Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "compare_results(results_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Varying Solver Type\n",
    "\n",
    "DEIS supports different solver types for polynomial interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different solver types\n",
    "solver_types = [\"logrho\", \"midpoint\", \"heun\", \"bh1\", \"bh2\"]\n",
    "NUM_STEPS = 20\n",
    "\n",
    "results_solver_type = {}\n",
    "\n",
    "for solver_type in solver_types:\n",
    "    print(f\"\\nSampling with solver_type='{solver_type}'...\")\n",
    "    \n",
    "    try:\n",
    "        scheduler = create_deis_scheduler(\n",
    "            num_train_timesteps=TIMESTEPS,\n",
    "            solver_order=2,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "            solver_type=solver_type,\n",
    "        )\n",
    "        \n",
    "        samples, elapsed = sample_with_deis(\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            num_inference_steps=NUM_STEPS,\n",
    "            batch_size=16,\n",
    "            image_size=IMAGE_SIZE,\n",
    "            channels=CHANNELS,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        results_solver_type[solver_type] = {\n",
    "            'samples': samples,\n",
    "            'time': elapsed\n",
    "        }\n",
    "        print(f\"  Time: {elapsed:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if results_solver_type:\n",
    "    compare_results(results_solver_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: Lower Order Final Steps\n",
    "\n",
    "Test the effect of using lower order solver at final denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lower_order_final setting\n",
    "lower_order_options = [True, False]\n",
    "NUM_STEPS = 20\n",
    "\n",
    "results_lower_order = {}\n",
    "\n",
    "for lower_order in lower_order_options:\n",
    "    print(f\"\\nSampling with lower_order_final={lower_order}...\")\n",
    "    \n",
    "    scheduler = create_deis_scheduler(\n",
    "        num_train_timesteps=TIMESTEPS,\n",
    "        solver_order=3,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        lower_order_final=lower_order,\n",
    "    )\n",
    "    \n",
    "    samples, elapsed = sample_with_deis(\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "        num_inference_steps=NUM_STEPS,\n",
    "        batch_size=16,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        channels=CHANNELS,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    results_lower_order[f\"lower_order_final={lower_order}\"] = {\n",
    "        'samples': samples,\n",
    "        'time': elapsed\n",
    "    }\n",
    "    print(f\"  Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "compare_results(results_lower_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 6: Visualize Denoising Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with intermediate steps\n",
    "scheduler = create_deis_scheduler(\n",
    "    num_train_timesteps=TIMESTEPS,\n",
    "    solver_order=2,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    ")\n",
    "\n",
    "samples, intermediates, elapsed = sample_with_deis(\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    num_inference_steps=20,\n",
    "    batch_size=4,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    channels=CHANNELS,\n",
    "    device=device,\n",
    "    return_intermediates=True\n",
    ")\n",
    "\n",
    "print(f\"Sampling time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize denoising process\n",
    "show_denoising_process(intermediates, num_steps_to_show=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 7: Comparison with Baseline DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DEIS with baseline DDIM sampling from the original model\n",
    "NUM_STEPS = 20\n",
    "\n",
    "# DEIS sampling\n",
    "print(\"Sampling with DEIS...\")\n",
    "scheduler_deis = create_deis_scheduler(\n",
    "    num_train_timesteps=TIMESTEPS,\n",
    "    solver_order=2,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    ")\n",
    "\n",
    "deis_samples, deis_time = sample_with_deis(\n",
    "    unet=unet,\n",
    "    scheduler=scheduler_deis,\n",
    "    num_inference_steps=NUM_STEPS,\n",
    "    batch_size=16,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    channels=CHANNELS,\n",
    "    device=device\n",
    ")\n",
    "print(f\"  DEIS time: {deis_time:.2f}s\")\n",
    "\n",
    "# Original DDIM sampling (from diffusion model)\n",
    "print(\"Sampling with original DDIM...\")\n",
    "ema_model.sampling_timesteps = NUM_STEPS\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    ddim_samples = ema_model.sample(batch_size=16)\n",
    "ddim_time = time.time() - start_time\n",
    "print(f\"  DDIM time: {ddim_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "comparison_results = {\n",
    "    f\"DEIS ({NUM_STEPS} steps)\": {'samples': deis_samples, 'time': deis_time},\n",
    "    f\"DDIM ({NUM_STEPS} steps)\": {'samples': ddim_samples, 'time': ddim_time},\n",
    "}\n",
    "\n",
    "compare_results(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 8: Speed vs Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive benchmark\n",
    "step_counts_benchmark = [5, 10, 15, 20, 30, 50]\n",
    "orders_benchmark = [1, 2, 3]\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "for order in orders_benchmark:\n",
    "    for num_steps in step_counts_benchmark:\n",
    "        scheduler = create_deis_scheduler(\n",
    "            num_train_timesteps=TIMESTEPS,\n",
    "            solver_order=order,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "        )\n",
    "        \n",
    "        # Run multiple times for accurate timing\n",
    "        times = []\n",
    "        for _ in range(3):\n",
    "            _, elapsed = sample_with_deis(\n",
    "                unet=unet,\n",
    "                scheduler=scheduler,\n",
    "                num_inference_steps=num_steps,\n",
    "                batch_size=16,\n",
    "                image_size=IMAGE_SIZE,\n",
    "                channels=CHANNELS,\n",
    "                device=device\n",
    "            )\n",
    "            times.append(elapsed)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        benchmark_results.append({\n",
    "            'order': order,\n",
    "            'steps': num_steps,\n",
    "            'time': avg_time\n",
    "        })\n",
    "        print(f\"Order {order}, {num_steps} steps: {avg_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot benchmark results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for order in orders_benchmark:\n",
    "    data = df[df['order'] == order]\n",
    "    ax.plot(data['steps'], data['time'], marker='o', label=f'Order {order}')\n",
    "\n",
    "ax.set_xlabel('Number of Steps')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('DEIS Sampling Time vs Number of Steps')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 9: FID-Based Quality Comparison\n",
    "\n",
    "Compare different DEIS configurations using FID score for quantitative evaluation. Lower FID = better quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for FID evaluation\n",
    "REAL_IMAGES_DIR = \"./data/val\"  # Path to real validation images (update this!)\n",
    "NUM_FID_SAMPLES = 1000  # Number of samples to generate for FID computation\n",
    "FID_OUTPUT_DIR = Path(\"./results/fid_evaluation\")\n",
    "FID_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Will generate {NUM_FID_SAMPLES} samples for each configuration\")\n",
    "print(f\"Real images directory: {REAL_IMAGES_DIR}\")\n",
    "print(f\"Output directory: {FID_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_for_fid(\n",
    "    unet, \n",
    "    scheduler_config: dict, \n",
    "    num_samples: int,\n",
    "    num_inference_steps: int,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    \"\"\"Generate samples for FID evaluation.\"\"\"\n",
    "    scheduler = create_deis_scheduler(**scheduler_config)\n",
    "    \n",
    "    all_samples = []\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Generating samples\"):\n",
    "        current_batch_size = min(batch_size, num_samples - i * batch_size)\n",
    "        samples = sample_with_deis(\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            batch_size=current_batch_size,\n",
    "            image_size=IMAGE_SIZE,\n",
    "            channels=CHANNELS,\n",
    "            device=device\n",
    "        )\n",
    "        all_samples.append(samples.cpu())\n",
    "    \n",
    "    return torch.cat(all_samples, dim=0)[:num_samples]\n",
    "\n",
    "\n",
    "def evaluate_config_fid(\n",
    "    config_name: str,\n",
    "    scheduler_config: dict,\n",
    "    num_inference_steps: int,\n",
    "    num_samples: int = NUM_FID_SAMPLES,\n",
    "):\n",
    "    \"\"\"Evaluate a DEIS configuration using FID score.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {config_name}\")\n",
    "    print(f\"Steps: {num_inference_steps}, Samples: {num_samples}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate samples\n",
    "    start_time = time.time()\n",
    "    samples = generate_samples_for_fid(\n",
    "        unet=unet,\n",
    "        scheduler_config=scheduler_config,\n",
    "        num_samples=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "    )\n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Save samples to directory\n",
    "    config_dir = FID_OUTPUT_DIR / config_name.replace(\" \", \"_\").replace(\"=\", \"_\")\n",
    "    save_samples_to_dir(samples, config_dir)\n",
    "    \n",
    "    # Compute FID\n",
    "    print(\"Computing FID score...\")\n",
    "    fid_score = compute_fid(REAL_IMAGES_DIR, str(config_dir), device=device)\n",
    "    \n",
    "    print(f\"Results for {config_name}:\")\n",
    "    print(f\"  - Generation time: {gen_time:.2f}s ({num_samples/gen_time:.1f} samples/sec)\")\n",
    "    print(f\"  - FID Score: {fid_score:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"config\": config_name,\n",
    "        \"steps\": num_inference_steps,\n",
    "        \"fid\": fid_score,\n",
    "        \"gen_time\": gen_time,\n",
    "        \"samples_per_sec\": num_samples / gen_time,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations to evaluate\n",
    "# Each config: (name, scheduler_config, num_steps)\n",
    "fid_configs = [\n",
    "    # Varying inference steps with Order 2\n",
    "    (\"DEIS Order2 10steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 2, \"beta_schedule\": \"scaled_linear\"}, 10),\n",
    "    (\"DEIS Order2 20steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 2, \"beta_schedule\": \"scaled_linear\"}, 20),\n",
    "    (\"DEIS Order2 50steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 2, \"beta_schedule\": \"scaled_linear\"}, 50),\n",
    "    \n",
    "    # Varying solver order with 20 steps\n",
    "    (\"DEIS Order1 20steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 1, \"beta_schedule\": \"scaled_linear\"}, 20),\n",
    "    (\"DEIS Order3 20steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 3, \"beta_schedule\": \"scaled_linear\"}, 20),\n",
    "    \n",
    "    # Different beta schedules with Order 2, 20 steps\n",
    "    (\"DEIS linear 20steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 2, \"beta_schedule\": \"linear\"}, 20),\n",
    "    (\"DEIS cosine 20steps\", {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 2, \"beta_schedule\": \"squaredcos_cap_v2\"}, 20),\n",
    "]\n",
    "\n",
    "print(f\"Will evaluate {len(fid_configs)} configurations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FID evaluation for all configurations\n",
    "fid_results = []\n",
    "\n",
    "for config_name, scheduler_config, num_steps in fid_configs:\n",
    "    try:\n",
    "        result = evaluate_config_fid(\n",
    "            config_name=config_name,\n",
    "            scheduler_config=scheduler_config,\n",
    "            num_inference_steps=num_steps,\n",
    "            num_samples=NUM_FID_SAMPLES,\n",
    "        )\n",
    "        fid_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {config_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FID Evaluation Complete!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "fid_df = pd.DataFrame(fid_results)\n",
    "fid_df = fid_df.sort_values(\"fid\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FID SCORES COMPARISON (sorted by FID, lower is better)\")\n",
    "print(\"=\"*80)\n",
    "print(fid_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configuration\n",
    "best_config = fid_df.iloc[0]\n",
    "print(f\"\\nð Best Configuration: {best_config['config']}\")\n",
    "print(f\"   FID Score: {best_config['fid']:.2f}\")\n",
    "print(f\"   Inference Steps: {best_config['steps']}\")\n",
    "print(f\"   Speed: {best_config['samples_per_sec']:.1f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FID results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: FID Scores bar chart\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(fid_df)))  # Red=bad, Green=good\n",
    "bars = ax1.barh(fid_df['config'], fid_df['fid'], color=colors)\n",
    "ax1.set_xlabel('FID Score (lower is better)')\n",
    "ax1.set_title('FID Scores by Configuration')\n",
    "ax1.invert_yaxis()  # Best at top\n",
    "\n",
    "# Add value labels\n",
    "for bar, fid in zip(bars, fid_df['fid']):\n",
    "    ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{fid:.1f}', va='center', fontsize=9)\n",
    "\n",
    "# Plot 2: FID vs Speed trade-off\n",
    "ax2 = axes[1]\n",
    "scatter = ax2.scatter(fid_df['samples_per_sec'], fid_df['fid'], \n",
    "                       c=fid_df['steps'], cmap='viridis', s=100, edgecolors='black')\n",
    "ax2.set_xlabel('Generation Speed (samples/sec)')\n",
    "ax2.set_ylabel('FID Score (lower is better)')\n",
    "ax2.set_title('Quality vs Speed Trade-off')\n",
    "\n",
    "# Add labels to points\n",
    "for _, row in fid_df.iterrows():\n",
    "    ax2.annotate(f\"{row['steps']}s\", (row['samples_per_sec'], row['fid']),\n",
    "                textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax2, label='Inference Steps')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FID_OUTPUT_DIR / \"fid_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {FID_OUTPUT_DIR / 'fid_comparison.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare DEIS vs Baseline DDIM using FID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DEIS vs baseline DDIM using FID\n",
    "COMPARE_STEPS = 20\n",
    "COMPARE_SAMPLES = NUM_FID_SAMPLES\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "# DEIS sampling\n",
    "print(\"Generating DEIS samples for FID comparison...\")\n",
    "deis_scheduler_config = {\"num_train_timesteps\": TIMESTEPS, \"solver_order\": 2, \"beta_schedule\": \"scaled_linear\"}\n",
    "deis_result = evaluate_config_fid(\n",
    "    config_name=f\"DEIS (Order 2, {COMPARE_STEPS} steps)\",\n",
    "    scheduler_config=deis_scheduler_config,\n",
    "    num_inference_steps=COMPARE_STEPS,\n",
    "    num_samples=COMPARE_SAMPLES,\n",
    ")\n",
    "comparison_results.append(deis_result)\n",
    "\n",
    "# Baseline DDIM sampling\n",
    "print(\"\\nGenerating baseline DDIM samples for FID comparison...\")\n",
    "ddim_output_dir = FID_OUTPUT_DIR / f\"DDIM_{COMPARE_STEPS}steps\"\n",
    "ddim_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ema_model.sampling_timesteps = COMPARE_STEPS\n",
    "start_time = time.time()\n",
    "\n",
    "all_ddim_samples = []\n",
    "batch_size = 64\n",
    "num_batches = (COMPARE_SAMPLES + batch_size - 1) // batch_size\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"Generating DDIM samples\"):\n",
    "    current_batch_size = min(batch_size, COMPARE_SAMPLES - i * batch_size)\n",
    "    with torch.no_grad():\n",
    "        samples = ema_model.sample(batch_size=current_batch_size)\n",
    "    all_ddim_samples.append(samples.cpu())\n",
    "\n",
    "ddim_samples = torch.cat(all_ddim_samples, dim=0)[:COMPARE_SAMPLES]\n",
    "ddim_gen_time = time.time() - start_time\n",
    "\n",
    "# Save DDIM samples\n",
    "save_samples_to_dir(ddim_samples, ddim_output_dir)\n",
    "\n",
    "# Compute DDIM FID\n",
    "print(\"Computing DDIM FID score...\")\n",
    "ddim_fid = compute_fid(REAL_IMAGES_DIR, str(ddim_output_dir), device=device)\n",
    "\n",
    "comparison_results.append({\n",
    "    \"config\": f\"DDIM Baseline ({COMPARE_STEPS} steps)\",\n",
    "    \"steps\": COMPARE_STEPS,\n",
    "    \"fid\": ddim_fid,\n",
    "    \"gen_time\": ddim_gen_time,\n",
    "    \"samples_per_sec\": COMPARE_SAMPLES / ddim_gen_time,\n",
    "})\n",
    "\n",
    "print(f\"\\nDDIM Results:\")\n",
    "print(f\"  - Generation time: {ddim_gen_time:.2f}s\")\n",
    "print(f\"  - FID Score: {ddim_fid:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DEIS vs DDIM comparison\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEIS vs DDIM COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['fid'], width, label='FID Score', color='steelblue')\n",
    "ax2 = ax.twinx()\n",
    "bars2 = ax2.bar(x + width/2, comparison_df['samples_per_sec'], width, label='Speed (samples/sec)', color='coral')\n",
    "\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('FID Score (lower is better)', color='steelblue')\n",
    "ax2.set_ylabel('Speed (samples/sec)', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['config'], rotation=15, ha='right')\n",
    "ax.set_title(f'DEIS vs DDIM Comparison ({COMPARE_STEPS} steps, {COMPARE_SAMPLES} samples)')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{height:.1f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "plt.tight_layout()\n",
    "plt.savefig(FID_OUTPUT_DIR / \"deis_vs_ddim_fid.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "deis_fid = comparison_df[comparison_df['config'].str.contains('DEIS')]['fid'].values[0]\n",
    "ddim_fid = comparison_df[comparison_df['config'].str.contains('DDIM')]['fid'].values[0]\n",
    "improvement = ((ddim_fid - deis_fid) / ddim_fid) * 100\n",
    "\n",
    "print(f\"\\nð Summary:\")\n",
    "print(f\"   DEIS FID: {deis_fid:.2f}\")\n",
    "print(f\"   DDIM FID: {ddim_fid:.2f}\")\n",
    "if deis_fid < ddim_fid:\n",
    "    print(f\"   â DEIS is {improvement:.1f}% better than DDIM in FID score\")\n",
    "else:\n",
    "    print(f\"   â ï¸ DDIM performs {-improvement:.1f}% better than DEIS in FID score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Best Configuration Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-quality samples with optimal configuration\n",
    "OUTPUT_DIR = Path(\"./results/deis_samples\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Best configuration (adjust based on experiments)\n",
    "scheduler = create_deis_scheduler(\n",
    "    num_train_timesteps=TIMESTEPS,\n",
    "    solver_order=2,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    solver_type=\"logrho\",\n",
    "    lower_order_final=True,\n",
    ")\n",
    "\n",
    "samples, elapsed = sample_with_deis(\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    num_inference_steps=50,\n",
    "    batch_size=64,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    channels=CHANNELS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Generated 64 samples in {elapsed:.2f}s\")\n",
    "\n",
    "# Save grid\n",
    "save_image(samples, OUTPUT_DIR / \"deis_samples_grid.png\", nrow=8)\n",
    "print(f\"Saved samples to {OUTPUT_DIR}\")\n",
    "\n",
    "# Show samples\n",
    "show_samples(samples, title=\"DEIS Samples (Order 2, 50 steps)\", nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key findings from DEIS experiments:\n",
    "\n",
    "### Visual Quality Observations\n",
    "\n",
    "1. **Number of Steps**: More steps generally produce better quality but take longer. DEIS can achieve good quality with 20-50 steps.\n",
    "\n",
    "2. **Solver Order**: Higher order (2 or 3) usually gives better results for a fixed number of steps. Order 2 is a good balance.\n",
    "\n",
    "3. **Beta Schedule**: The schedule should ideally match what was used during training. `scaled_linear` is a common choice.\n",
    "\n",
    "4. **Solver Type**: `logrho` is the default and works well for most cases.\n",
    "\n",
    "5. **Lower Order Final**: Using lower order at final steps can improve stability for high-order solvers.\n",
    "\n",
    "### FID-Based Quantitative Findings\n",
    "\n",
    "6. **FID Scores**: The FID metric provides objective quality comparison. Lower FID indicates better sample quality closer to the real data distribution.\n",
    "\n",
    "7. **DEIS vs DDIM**: DEIS typically achieves comparable or better FID scores than baseline DDIM with the same number of steps, validating its effectiveness as a fast sampler.\n",
    "\n",
    "8. **Speed-Quality Trade-off**: The FID vs speed analysis helps identify optimal configurations that balance generation quality with computational efficiency.\n",
    "\n",
    "### Recommended Configuration\n",
    "\n",
    "Based on FID evaluation, the recommended DEIS configuration is:\n",
    "- **Solver Order**: 2 (good balance of quality and stability)\n",
    "- **Inference Steps**: 20-50 (depending on quality requirements)\n",
    "- **Beta Schedule**: Match training schedule (typically `scaled_linear` or `sigmoid`)\n",
    "- **Solver Type**: `logrho` (default)\n",
    "- **Lower Order Final**: `True` (improves stability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif-accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
